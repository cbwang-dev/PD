{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1cmZnQOm517"
   },
   "source": [
    "## P&O ISSP: Brain-computer interface voor sturing van een directionele akoestische zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15TgxRRrm52A"
   },
   "source": [
    "In this notebook, we will start building a basic deep learning implementation for classifying which two of the Stimuli was attended to, when given EEG and both Stimuli as input. \n",
    "\n",
    "\n",
    "One of the ways to process the EEG data is to find specific patterns in the signal. Based on the presence or absence of these patterns we will decide where is the auditory attention. But handcrafting these pattern might be difficult, so we will use convolutional neural network to learn filters which can detect those patterns.\n",
    "\n",
    "The implementation will be in mulitple phases. First, we will get more familiar with keras and the deep learning framework by mimicking the linear regression-based network, but then in a non-linear context. Once we have implemented this, we can start playing with the deep learning architectures and add some blocks, see what different training schemes do, etc... \n",
    "\n",
    "The following paper has a very similar approach to the one we will take. Note that there is a difference with the data. In the paper, there is always only 1 speaker that is talking while the EEG is recorded, whereas we have 2 competing speakers. However, the implementation of the networks will be the same. Instead of taking the mismatch in the future of the same envelope, we already have an attended and unattended envelope here. \n",
    "\n",
    "Accou, Bernd, et al. \"Modeling the relationship between acoustic stimulus and EEG with a dilated convolutional neural network.\" 2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021.\n",
    "\n",
    "If we look at the paper, they compare three different methods. We have alread implemented method a) Linear decoder baseline. \n",
    "We will now first implement method b) Convolutional baseline method and then expand from this towards c) dilation model or d) your own extentions on this model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJUj1LWm52C"
   },
   "source": [
    "**Note**: If keras is  not already installed, execute: !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_xoiZWUm52E"
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Activation\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRUN1nnem52N"
   },
   "source": [
    "![fig_1](dilation-figures.png)\n",
    "\n",
    "* The EEG data preprocessing has been explained in another tutorial.\n",
    "* we have already implemeted method a) linear decoder baseline \n",
    "* we will now start with method b) convolutional baseline method \n",
    "* once this is implemented, we can expand this model towards model c) dilation model and possibly add extentions \n",
    "\n",
    "**Convolutional baseline network**\n",
    "* The first step in the model is a convolutional layer, indicated in red. A (64 x 16) spatio-temporal filter is shifted over the input matrix, containing the EEG.\n",
    "* A rectifying linear unit (ReLu) activation function is used after the convolution step. the kernel size of 16 is chosen because, as is the case in the linear model, we want to look to future EEG to predict the current envelope. the EEG is sampled at fs=64Hz, giving us a temporal resolution of 16/64 = 250ms.\n",
    "* The output of the convolutional block is a (time-window, 1) signal. \n",
    "* In the next step, we calculate the cosine similarity between this signal and both of the envelopes. We will calculate this cosine similarity by applying a *dot product* between the signal and both envelopes. \n",
    "* As a last step, we then have to choose which one of the two attended envelopes is the one we want to choose. We do this by applying a single neuron ( **dense layer** in keras, with a sigmoid activation function. \n",
    "\n",
    "**dilation model**\n",
    "* the idea here is the same. We still give EEG and envelopes to the model, there are just more processing steps in between before we have to make a decision. \n",
    "* we first apply a one-dimensional convolution to the EEG, with 8 output filters. We can interpret this as kind of a non-linear dimensionality reduction, as the resulting EEG has shape (time-window, 8) instead of the original (time-window, 64) \n",
    "* next, there are some dilated convolutional blocks. these blocks will perform convolutions, with a certain dilation factor ( see picture). This enlarges the receptive field of the convolution, while still keeping the number of parameters small. Eg. we can look over a longer time period. These convolutions are applied to both EEG and envelopes. \n",
    "* after that, we once again compute the dot product and subsequently put the result of this in a sigmoid neuron to reach an end decision. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hmbrPpkm52O"
   },
   "outputs": [],
   "source": [
    "\n",
    "eeg = tf.keras.layers.Input(shape=[time_window, 64])\n",
    "env1 = tf.keras.layers.Input(shape=[time_window, 1])\n",
    "env2 = tf.keras.layers.Input(shape=[time_window, 1])\n",
    "\n",
    "#add model layers\n",
    "## ---- add your code ----here\n",
    "\n",
    "# Classification\n",
    "out1 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(\n",
    "    tf.keras.layers.Flatten()(tf.keras.layers.Concatenate()([cos1, cos2])))\n",
    "\n",
    "# 1 output per batch\n",
    "out = tf.keras.layers.Reshape([1], name=output_name)(out1)\n",
    "model = tf.keras.Model(inputs=[eeg, env1, env2], outputs=[out])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvReB1npm52U"
   },
   "outputs": [],
   "source": [
    "# To check the model summary:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training the model, we need to make sure that the data is equally balanced. We have attended and unattended envelopes that we give to the model. If we always put the attended envelope at stream 1 and the unattended at stream 2, the model will quickly figure out that it should just always output stream 1 and hence not learn anything. \n",
    "\n",
    "The solution to this is to present each segment of EEG twice, where we swap the envelopes, ( and thus, the labels), from place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_equalizer(eeg, env_1, env_2, labels):\n",
    "    # present each of the eeg segments twice, where the envelopes, and thus the labels \n",
    "    # are swapped around. EEG presented in small segments [bs, window_length, 64]\n",
    "    return (np.concatenate([eeg,eeg], axis=0), np.concatenate([env_1, env_2], axis=0),np.concatenate([ env_2, env_1], axis=0)), np.concatenate([labels, (labels+1)%2], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3mzuy-jm52i"
   },
   "source": [
    "* Now we prepare our data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3X0Vn9JWm52k"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# To load the mat file in v7.3 format. For all previous formats use scipy.io loadmat (https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html)\n",
    "def load_large_mat(filepath):\n",
    "    arrays = {}\n",
    "    f = h5py.File(filepath)\n",
    "    for k, v in f.items():\n",
    "        arrays[k] = np.array(v)\n",
    "    f.close()\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuPGMqTKm52p"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def fn_all(fnarrays1,fnarrays2):\n",
    "    #------add your prerprocessing steps here\n",
    "    fnxtr_all = fnarrays1\n",
    "    fny_tr_all = fnarrays2\n",
    "    x = np.expand_dims(fnxtr_all,-1)\n",
    "    \n",
    "    return x,y"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_S_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
