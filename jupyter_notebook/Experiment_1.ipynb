{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_1.ipynb","provenance":[],"collapsed_sections":["dEr7LWrtfx10","5EbkE99eaZwa","_5rPEu9QWwxe","w-4OiJcVHX6q","xC_xtHboFOzm","wV3_yevVbAQX"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DJSDRfZTXojn"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"_BR-SVJFX0VF"},"source":["[1]: Deckers, Lucas, et al. \"EEG-based detection of the attended speaker and the locus of auditory attention with convolutional neural networks.\" bioRxiv (2018): 475673."]},{"cell_type":"markdown","metadata":{"id":"H-FQSSO_ub71"},"source":["# Library and File Imports"]},{"cell_type":"code","metadata":{"id":"i9q7UVZGbvwx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619361786489,"user_tz":-120,"elapsed":6068,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"2b434f99-50f6-4330-cf89-08de4cb8a16a"},"source":["from google.colab import drive\n","drive.mount('gdrive', force_remount=True)\n","\n","import sys\n","import os\n","sys.path.append(os.path.abspath('/content/gdrive/MyDrive/code/Phase_2_Part_2_CNN'))\n","# after changing any python file, Runtime -> Restart and run all\n","from dataGenerator import dataGenerator \n","# from network import baseline_model\n","from sklearn.model_selection import train_test_split\n","from scipy.io import loadmat\n","import numpy as np\n","import pickle\n","from random import randint\n","import tensorflow as tf\n","import pandas as pd\n","import scipy.io as sio\n","import h5py\n","from scipy.io import loadmat\n","import keras\n","import keras.backend as K\n","from keras.initializers import RandomUniform\n","from keras.models import Sequential, load_model\n","from keras.utils import to_categorical\n","from keras import regularizers\n","from keras.callbacks import LearningRateScheduler\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import Input, Dense, Softmax, Activation, BatchNormalization, \\\n","                         Conv2D, Flatten, GlobalAveragePooling2D, Dropout\n","from keras.models import Model"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dEr7LWrtfx10"},"source":["# Parameters Definition"]},{"cell_type":"markdown","metadata":{"id":"5EbkE99eaZwa"},"source":["## File Related Parameters"]},{"cell_type":"code","metadata":{"id":"uoAJzdPnaoVu","executionInfo":{"status":"ok","timestamp":1619361789415,"user_tz":-120,"elapsed":514,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["number_of_subjects   = 16\n","tag_EEG              = 'output_Eeg'     # in case if switched in the future\n","tag_stimuli          = 'Stimuli_eeg'    # in case if switched in the future\n","tag_attended_track   = 'Attended_track' # in case if switched in the future\n","EEG_dir              = r'/content/gdrive/MyDrive/data/EEG_data_Nicolas/' \n","EEG_test_dir         = None             # with no test set given\n","audio_dir            = r'/content/gdrive/MyDrive/data/Audio_data/'\n","model_save_dir       = r'/content/gdrive/MyDrive/results/Phase_2/CNN/model/'\n","experiment_folder    = r'experiment_1/' # modify if you change the experiment\n","experiment_save_dir  = model_save_dir + experiment_folder\n","checkpoint_save_dir  = experiment_save_dir + 'checkpoints/'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5rPEu9QWwxe"},"source":["## Dataset Generation Related Parameters"]},{"cell_type":"code","metadata":{"id":"RwnY2RpgW0jE","executionInfo":{"status":"ok","timestamp":1619361793407,"user_tz":-120,"elapsed":488,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["window_time = 10 # denoted in seconds. \n","# choices of window time: [10, 5, 2, 1] denoted in paper .\n","\n","verbose = True # display information, True for more information. \n","\n","bool_validation = True # whether to split validation set from the dataset.\n","\n","bool_test_set = False # \n","\n","'''whether to add the preprocessed audio envelope onto the network's input data.\n","if True, then the number of channels of one sample will be [window_length, 66], \n","else, will be [window_length, 64]. \n","'''\n","bool_envelope = True\n","\n","'''the ratio between the overlapping part of the windows and the length of the \n","window. If the ratio is higher, the more samples you will get. However, due to \n","the limited RAM of Colaboratory, the maximum value of the window is around 50%. \n","'''\n","window_overlap_ratio = 0\n","\n","'''if the network structure and training strategy is checked as usable, switch \n","the leave_out_index_validation parameter to other values from 0 to 7, thus \n","conducting a leave-one-out validation for the model. As a result, we don't need \n","to split the test data out from the dataset.\n","''' \n","leave_out_index_validation = 0 \n","\n","'''due to the nature, the EEG signal at time `t` is aligned to the audio signal\n","at time `t-delay`. \n","'''\n","envelope_delay_time = 0.1 # denoted in seconds\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNHvWIYCYpH7"},"source":["## Network Related Parameters"]},{"cell_type":"code","metadata":{"id":"FP-fZLwBdYQm","executionInfo":{"status":"ok","timestamp":1619361796460,"user_tz":-120,"elapsed":1203,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["batch_size                 = 60\n","epochs                     = 200 # never exceeds 60, indication.\n","bool_normalization         = True # normalizing the input data or not. "],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"utKLz67iaw6I"},"source":["## Compute on Other Parameters"]},{"cell_type":"code","metadata":{"id":"YLV4igj-a5PD","executionInfo":{"status":"ok","timestamp":1619361797824,"user_tz":-120,"elapsed":929,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["downsample_frequency = 70\n","window_length = int(window_time * downsample_frequency)\n","envelope_delay = int(envelope_delay_time * downsample_frequency)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHRgtqMtf0vC"},"source":["# Reshape Matlab Data Into Trainable Data"]},{"cell_type":"code","metadata":{"id":"AUa6r-C-H1tX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619362764484,"user_tz":-120,"elapsed":39224,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"b820ad44-ffd0-42ce-c304-21060323b9d0"},"source":["# ===== data generation START ===== #\n","X_train, Y_train, X_validation, Y_validation = \\\n","    dataGenerator(window_overlap_ratio=window_overlap_ratio,\n","        window_length=window_length, envelope_delay=envelope_delay, verbose=0,\n","        tag_EEG=tag_EEG, tag_stimuli=tag_stimuli, tag_attended_track=tag_attended_track,\n","        EEG_dir=EEG_dir, EEG_test_dir=EEG_test_dir, audio_dir=audio_dir,\n","        bool_validation=bool_validation, bool_test_set=bool_test_set, bool_envelope=bool_envelope,\n","        leave_out_index_validation=leave_out_index_validation)\n","# ===== data generation END ===== #"],"execution_count":9,"outputs":[{"output_type":"stream","text":["INFO: Start generating training/validation set with envelope\n","====> pre_process02_CNN.mat: processing start\n","====> pre_process03_CNN.mat: processing start\n","====> pre_process04_CNN.mat: processing start\n","====> pre_process05_CNN.mat: processing start\n","====> pre_process06_CNN.mat: processing start\n","====> pre_process07_CNN.mat: processing start\n","====> pre_process08_CNN.mat: processing start\n","====> pre_process09_CNN.mat: processing start\n","====> pre_process10_CNN.mat: processing start\n","====> pre_process11_CNN.mat: processing start\n","====> pre_process12_CNN.mat: processing start\n","====> pre_process13_CNN.mat: processing start\n","====> pre_process14_CNN.mat: processing start\n","====> pre_process15_CNN.mat: processing start\n","====> pre_process16_CNN.mat: processing start\n","====> pre_process17_CNN.mat: processing start\n","INFO: Finished dataset generation.\n","====> X dims AFTER adding extra dim (for keras)\n","      X_train      is (3696, 700, 66, 1)\n","      X_validation is (528, 700, 66, 1)\n","====> Y dims after applying to_categorical\n","      Y_train      is (3696, 2)\n","      Y_validation is (528, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NOPm-zrCUayh"},"source":["## check whether the outputs are aligned properly\n","\n"]},{"cell_type":"code","metadata":{"id":"-TX_zubdTkec"},"source":["if verbose:\n","  sign = 0 # 0 and 1, left or right\n","  temp=Y_train # Y_validation and Y_train\n","  print(\"Printing the output of dataset in order...\")\n","  for i in range(min(1000,temp.shape[0])):\n","    if int(temp[i-1][sign]) != int(temp[i][sign]):\n","      print(\"\")\n","    print(int(temp[i][sign]),end='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tNvPllNlM7ja"},"source":["## check whether the inputs are aligned (or seperated) correctly\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eBut0NeLM66m","executionInfo":{"status":"ok","timestamp":1619164221570,"user_tz":-120,"elapsed":1418,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"de73e0ca-1abf-4e34-ef40-80ce48fd1f79"},"source":["if window_overlap_ratio == 0:\n","  if leave_out_index_validation == 0:\n","    # print(X_validation.shape)\n","    X_validation_generated = np.reshape(X_validation[0][0],X_validation[0][0].shape[0])\n","    # print(X_validation_generated.shape)\n","    print(X_validation_generated[1:7])\n","    filename='pre_process02_CNN.mat'\n","    EEG_all = loadmat(EEG_dir + '/' + filename)\n","    output_Eeg = EEG_all['output_Eeg']\n","    # print(output_Eeg[leave_out_index_validation][0].shape)\n","    print(output_Eeg[leave_out_index_validation][0][envelope_delay][0:6])\n","    if X_validation_generated[1:7].all() == output_Eeg[leave_out_index_validation][0][7][0:6].all():\n","      print(\"The data is aligned when trial \" + str(leave_out_index_validation+1) + \n","            \" is read into the validation set. \")\n","    del filename, EEG_all, output_Eeg, X_validation_generated"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.07177847 -0.10532329 -0.07419017 -0.03526331 -0.01319777 -0.15567087]\n","[-0.07177847 -0.10532329 -0.07419017 -0.03526331 -0.01319777 -0.15567087]\n","The data is aligned when trial 1 is read into the validation set. \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"miwbJ_yB51qD"},"source":["# Normalizing Input Data"]},{"cell_type":"code","metadata":{"id":"HoUvuvGC0EwV","executionInfo":{"status":"ok","timestamp":1619362887389,"user_tz":-120,"elapsed":5472,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["# normalize input data wrt #channels\n","# mean, max and min are [66, 700, 1] dimensional vector.\n","if bool_normalization:\n","  mega_x = np.concatenate((X_train,X_validation),axis=0)\n","  X_train[:,:,:,:] = (X_train[:,:,:,:] - np.mean(mega_x[:,:,:,:],axis=(0,3),keepdims=True))/np.std(mega_x[:,:,:,:],axis=(0,3),keepdims=True)\n","  X_validation[:,:,:,:] = (X_validation [:,:,:,:] - np.mean(mega_x[:,:,:,:],axis=(0,3),keepdims=True))/np.std(mega_x[:,:,:,:],axis=(0,3),keepdims=True)\n","  #for i in range(X_train.shape[0]):\n","  #  X_train[i,:,:,:] = (X_train[i,:,:,:] - np.mean(X_train[i,:,:,:]))/(np.max(X_train[i,:,:,:]) - np.min(X_train[i,:,:,:]))\n","  #for i in range(X_validation.shape[0]):\n","  #  X_validation[i,:,:,:] = (X_validation[i,:,:,:] - np.mean(X_validation[i,:,:,:]))/(np.max(X_validation[i,:,:,:]) - np.min(X_validation[i,:,:,:]))\n","  #print('the mean of input (training set) is: ' + str(np.mean(X_train)) + \n","  #      '\\nthe standard deviation of input (training set) is: ' + str(np.std(X_train)))\n","  #print('the mean of input (validation set) is: ' + str(np.mean(X_validation)) + \n","  #      ',\\nthe standard deviation of input (validation set) is: ' + str(np.std(X_validation)))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ot-af0VBNxqE"},"source":["# Network Related Functions"]},{"cell_type":"markdown","metadata":{"id":"Z0XLsue7OH7D"},"source":["## Callbacks"]},{"cell_type":"markdown","metadata":{"id":"sScDHzFzf_tC"},"source":["### Learning Rate Scheduler Definition\n","\n"]},{"cell_type":"code","metadata":{"id":"PkJWqKji5INC","executionInfo":{"status":"ok","timestamp":1619362897454,"user_tz":-120,"elapsed":569,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["class CustomLearningRateScheduler(keras.callbacks.Callback):\n","\n","    def __init__(self, schedule):\n","        super(CustomLearningRateScheduler, self).__init__()\n","        self.schedule = schedule\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        if not hasattr(self.model.optimizer, \"lr\"):\n","            raise ValueError('Optimizer must have a \"lr\" attribute.')\n","        # Get the current learning rate from model's optimizer.\n","        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n","        # Call schedule function to get the scheduled learning rate.\n","        scheduled_lr = self.schedule(epoch, lr)\n","        # Set the value back to the optimizer before this epoch starts\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n","        print(\"Learning rate is %6.4f.\" % (scheduled_lr))\n","\n","LR_SCHEDULE = [\n","    # (epoch to start, learning rate) tuples\n","    (0, 0.1),\n","    (10, 0.05),\n","    (25, 0.025),\n","    (40, 0.0125),\n","] \n","# Used in pretraining, updated in subject-specific retraining. The learning rate\n","# is divided by 10 during subject-specific retraining. \n","\n","def lr_schedule(epoch, lr):\n","    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n","        return lr\n","    for i in range(len(LR_SCHEDULE)):\n","        if epoch == LR_SCHEDULE[i][0]:\n","          return LR_SCHEDULE[i][1]\n","    return lr # if loss stagnate, return lr * 0.1"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pEyg7iyT5M5A"},"source":["### Early Stopping Strategy Definition"]},{"cell_type":"code","metadata":{"id":"l8ugOgaaFtTR","executionInfo":{"status":"ok","timestamp":1619362899848,"user_tz":-120,"elapsed":570,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["early_stopping = EarlyStopping(\n","                                monitor=\"val_loss\",\n","                                min_delta=0, # no loss reduction\n","                                patience=10, # 10 non-consecutive epochs\n","                                verbose=1,\n","                                mode=\"min\",\n","                                restore_best_weights=True,\n","                                )"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAXnAAEEPiWv"},"source":["### Model Checkpoint Definition"]},{"cell_type":"code","metadata":{"id":"DO5vbk8HPj-3","executionInfo":{"status":"ok","timestamp":1619362902976,"user_tz":-120,"elapsed":508,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["checkpoint = ModelCheckpoint(filepath = checkpoint_save_dir + 'pretrained_epoch_{epoch:02d}_valoss{val_loss:.2f}.h5',\n","                             monitor='val_loss', \n","                             verbose=verbose, \n","                             save_best_only=True, \n","                             mode='min')\n","# Checkpoints are all saved under the `/checkpoints/` folder. "],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCdgnz1Fxytq"},"source":["### Weight Printing Definition\n","\n","Print the mean and variance of the weights before and after each epoch. \n","\n"]},{"cell_type":"code","metadata":{"id":"YmIEFHqbx1Km"},"source":["class WeightCallback(tf.keras.callbacks.Callback):\n","  def on_train_batch_begin(self, batch, logs=None):\n","    conv_layer = model.get_layer('conv')\n","    conv_layer_weights = conv_layer.get_weights()\n","    # print(\"the shape of weights in conv layer is: (\" + \\\n","    #       str(len(conv_layer_weights)) + ',' + \\\n","    #       str(len(conv_layer_weights[0])) + ',' + \\\n","    #       str(len(conv_layer_weights[0][0])) + ',' + \\\n","    #       str(len(conv_layer_weights[0][0][0])) + ',' + \\\n","    #       str(len(conv_layer_weights[0][0][0][0])) + ').')\n","    conv_layer_weights = np.array(conv_layer_weights,dtype=object)[0]\n","    conv_layer_weights_flatten=conv_layer_weights.flatten()\n","    print('===> BEGIN batch: The mean of all weights is: ' + \n","          str(np.mean(conv_layer_weights_flatten)) + \n","          ', The variances of all weights is: ' + \n","          str(np.var(conv_layer_weights_flatten)))\n","  def on_train_batch_end(self, batch, logs=None):\n","    conv_layer = model.get_layer('conv')\n","    conv_layer_weights = conv_layer.get_weights()\n","    conv_layer_weights = np.array(conv_layer_weights,dtype=object)[0]\n","    conv_layer_weights_flatten=conv_layer_weights.flatten()\n","    print('\\n===> END   batch: The mean of all weights is: ' + \n","          str(np.mean(conv_layer_weights_flatten)) + \n","          ', The variances of all weights is: ' + \n","          str(np.var(conv_layer_weights_flatten)))\n","  def on_test_batch_begin(self, batch, logs=None):\n","    pass\n","  def on_test_batch_end(self, batch, logs=None):\n","    pass\n","\n","weight_print = WeightCallback()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eYp-r74OPpeW"},"source":["### Callback List Generation"]},{"cell_type":"code","metadata":{"id":"W7qQdVmkPuUN","executionInfo":{"status":"ok","timestamp":1619362905921,"user_tz":-120,"elapsed":606,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["callbacks_list = [CustomLearningRateScheduler(lr_schedule), \n","                  checkpoint, \n","                  early_stopping] # weight_print"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1dxQACCMbdMu"},"source":["## Network Definition"]},{"cell_type":"code","metadata":{"id":"jae2hDyWmf9q","executionInfo":{"status":"ok","timestamp":1619363387574,"user_tz":-120,"elapsed":605,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}}},"source":["def baseline_model(window_length, verbose):\n","  initz = RandomUniform(minval=-0.1, maxval=0.1, seed=None) #intializer\n","  model = Sequential()\n","  model.add(Input(shape=[window_length,66,1]))\n","  model.add(Conv2D(filters=5, name='conv',\n","                  kernel_size=(9,66),\n","                  strides=(1,1),\n","                  padding='valid',\n","                  activation='relu',\n","                  ))\n","  model.add(GlobalAveragePooling2D(name='globalPooling'))\n","  model.add(Dense(units=5, name='FC1',\n","                  activation='sigmoid',\n","                  ))\n","  model.add(Dense(units=2, name='FC2',\n","                  activation='softmax',\n","                  ))\n","  model.summary()\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'],\n","                )\n","  return model"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-4OiJcVHX6q"},"source":["## Network Check: Can It Overfit on Small Size of Training Samples\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c-o_EAU_jObl"},"source":["This section is for the first sanity check. If the network cannot overfit to small training set (which means it cannot get a ~100% accuracy and a low loss), the network is needed to be redesigned. "]},{"cell_type":"markdown","metadata":{"id":"07UREySbO4Gp"},"source":["### When Training Samples (Small Size) are Generated Randomly (wrong)"]},{"cell_type":"code","metadata":{"id":"p24ErUenulM8"},"source":["size_test = batch_size \n","X_train_test = np.random.randn(size_test, window_length, 66, 1)\n","Y_train_test = np.random.randint(2, size=size_test)\n","Y_train_test = tf.keras.utils.to_categorical(Y_train_test,2)\n","model_test = baseline_model(window_length, verbose)\n","history = model_test.fit(X_train_test, Y_train_test,\n","                    batch_size=batch_size,\n","                    epochs=50,\n","                    verbose=2, \n","                    shuffle=True)\n","del size_test, X_train_test, Y_train_test, model_test, history\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bkhqyltZO_18"},"source":["### When Training Samples (Small Size) are Sampled from the Training Data"]},{"cell_type":"code","metadata":{"id":"fSPByvu-0vC2"},"source":["sample_ratio = 0.01 # how much data are sampled from trainings set\n","_, X_sample, _, Y_sample = train_test_split(X_train, Y_train, \n","                                            test_size=sample_ratio)\n","# print(X_sample[0][0:10][0:10][:])\n","# print(Y_sample)\n","model_test = baseline_model(window_length, 1)\n","history = model_test.fit(X_sample, Y_sample,\n","                    batch_size=50,\n","                    epochs=50,\n","                    verbose=2, \n","                    shuffle=True)\n","del sample_ratio, model_test, X_sample, Y_sample, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2INgNSgRgGKq"},"source":["# Pre-Training on Entire Training Data"]},{"cell_type":"markdown","metadata":{"id":"jtcZXDNERVWt"},"source":["## Pre-Training: Execution and Save Model in `pretrain.h5`\n"]},{"cell_type":"code","metadata":{"id":"5LKtWSZmFhvv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619363782701,"user_tz":-120,"elapsed":391115,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"08cd995b-f27a-46b5-ca36-c7754378ee57"},"source":["model = baseline_model(window_length, verbose)\n","history = model.fit(X_train, Y_train,\n","                              batch_size=50,\n","                              epochs=epochs,\n","                              verbose=1, \n","                              callbacks=callbacks_list,\n","                              validation_data=(X_validation, Y_validation), \n","                              shuffle=True)\n","scores = model.evaluate(X_validation, Y_validation, verbose=verbose)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","# save model in pretrain.h5\n","# filepath = model_save_dir + \"pretrain.h5\"\n","# model.save(filepath)\n","# del model"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv (Conv2D)                (None, 692, 1, 5)         2975      \n","_________________________________________________________________\n","globalPooling (GlobalAverage (None, 5)                 0         \n","_________________________________________________________________\n","FC1 (Dense)                  (None, 5)                 30        \n","_________________________________________________________________\n","FC2 (Dense)                  (None, 2)                 12        \n","=================================================================\n","Total params: 3,017\n","Trainable params: 3,017\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 13s 172ms/step - loss: 0.7109 - accuracy: 0.5053 - val_loss: 0.7004 - val_accuracy: 0.5000\n","\n","Epoch 00001: val_loss did not improve from 0.69328\n","Epoch 2/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 13s 169ms/step - loss: 0.6991 - accuracy: 0.4969 - val_loss: 0.6997 - val_accuracy: 0.5000\n","\n","Epoch 00002: val_loss did not improve from 0.69328\n","Epoch 3/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.6988 - accuracy: 0.4909 - val_loss: 0.6948 - val_accuracy: 0.5000\n","\n","Epoch 00003: val_loss did not improve from 0.69328\n","Epoch 4/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 13s 169ms/step - loss: 0.7091 - accuracy: 0.5067 - val_loss: 0.6974 - val_accuracy: 0.5000\n","\n","Epoch 00004: val_loss did not improve from 0.69328\n","Epoch 5/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 169ms/step - loss: 0.6966 - accuracy: 0.5145 - val_loss: 0.7027 - val_accuracy: 0.5000\n","\n","Epoch 00005: val_loss did not improve from 0.69328\n","Epoch 6/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 169ms/step - loss: 0.6976 - accuracy: 0.5241 - val_loss: 0.7020 - val_accuracy: 0.5000\n","\n","Epoch 00006: val_loss did not improve from 0.69328\n","Epoch 7/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.6941 - accuracy: 0.5233 - val_loss: 0.6960 - val_accuracy: 0.5000\n","\n","Epoch 00007: val_loss did not improve from 0.69328\n","Epoch 8/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 169ms/step - loss: 0.7082 - accuracy: 0.4883 - val_loss: 0.7139 - val_accuracy: 0.5000\n","\n","Epoch 00008: val_loss did not improve from 0.69328\n","Epoch 9/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.7049 - accuracy: 0.5049 - val_loss: 0.7003 - val_accuracy: 0.5000\n","\n","Epoch 00009: val_loss did not improve from 0.69328\n","Epoch 10/200\n","Learning rate is 0.1000.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.7041 - accuracy: 0.5092 - val_loss: 0.7020 - val_accuracy: 0.5000\n","\n","Epoch 00010: val_loss did not improve from 0.69328\n","Epoch 11/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 171ms/step - loss: 0.6976 - accuracy: 0.5029 - val_loss: 0.6952 - val_accuracy: 0.5000\n","\n","Epoch 00011: val_loss did not improve from 0.69328\n","Epoch 12/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 171ms/step - loss: 0.6993 - accuracy: 0.4870 - val_loss: 0.6932 - val_accuracy: 0.5000\n","\n","Epoch 00012: val_loss improved from 0.69328 to 0.69315, saving model to /content/gdrive/MyDrive/results/Phase_2/CNN/model/experiment_1/checkpoints/pretrained_epoch_12_valoss0.69.h5\n","Epoch 13/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 171ms/step - loss: 0.6955 - accuracy: 0.5013 - val_loss: 0.6948 - val_accuracy: 0.5000\n","\n","Epoch 00013: val_loss did not improve from 0.69315\n","Epoch 14/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 171ms/step - loss: 0.6993 - accuracy: 0.4940 - val_loss: 0.6968 - val_accuracy: 0.5000\n","\n","Epoch 00014: val_loss did not improve from 0.69315\n","Epoch 15/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 170ms/step - loss: 0.6950 - accuracy: 0.4996 - val_loss: 0.6943 - val_accuracy: 0.5000\n","\n","Epoch 00015: val_loss did not improve from 0.69315\n","Epoch 16/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 171ms/step - loss: 0.6957 - accuracy: 0.4918 - val_loss: 0.6933 - val_accuracy: 0.5000\n","\n","Epoch 00016: val_loss did not improve from 0.69315\n","Epoch 17/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 170ms/step - loss: 0.7000 - accuracy: 0.5053 - val_loss: 0.6982 - val_accuracy: 0.5000\n","\n","Epoch 00017: val_loss did not improve from 0.69315\n","Epoch 18/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 170ms/step - loss: 0.6978 - accuracy: 0.4990 - val_loss: 0.6973 - val_accuracy: 0.5000\n","\n","Epoch 00018: val_loss did not improve from 0.69315\n","Epoch 19/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 172ms/step - loss: 0.6956 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.5000\n","\n","Epoch 00019: val_loss did not improve from 0.69315\n","Epoch 20/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 170ms/step - loss: 0.6964 - accuracy: 0.4976 - val_loss: 0.6938 - val_accuracy: 0.5000\n","\n","Epoch 00020: val_loss did not improve from 0.69315\n","Epoch 21/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 12s 169ms/step - loss: 0.6943 - accuracy: 0.5068 - val_loss: 0.6931 - val_accuracy: 0.5000\n","\n","Epoch 00021: val_loss improved from 0.69315 to 0.69315, saving model to /content/gdrive/MyDrive/results/Phase_2/CNN/model/experiment_1/checkpoints/pretrained_epoch_21_valoss0.69.h5\n","Epoch 22/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.6954 - accuracy: 0.5100 - val_loss: 0.6947 - val_accuracy: 0.5000\n","\n","Epoch 00022: val_loss did not improve from 0.69315\n","Epoch 23/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.6952 - accuracy: 0.5015 - val_loss: 0.7077 - val_accuracy: 0.5000\n","\n","Epoch 00023: val_loss did not improve from 0.69315\n","Epoch 24/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.7072 - accuracy: 0.4756 - val_loss: 0.6933 - val_accuracy: 0.5000\n","\n","Epoch 00024: val_loss did not improve from 0.69315\n","Epoch 25/200\n","Learning rate is 0.0500.\n","74/74 [==============================] - 13s 170ms/step - loss: 0.6971 - accuracy: 0.5011 - val_loss: 0.7019 - val_accuracy: 0.5000\n","\n","Epoch 00025: val_loss did not improve from 0.69315\n","Epoch 26/200\n","Learning rate is 0.0250.\n","74/74 [==============================] - 13s 169ms/step - loss: 0.6979 - accuracy: 0.4948 - val_loss: 0.6950 - val_accuracy: 0.5000\n","\n","Epoch 00026: val_loss did not improve from 0.69315\n","Epoch 27/200\n","Learning rate is 0.0250.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.6963 - accuracy: 0.5101 - val_loss: 0.6975 - val_accuracy: 0.5000\n","\n","Epoch 00027: val_loss did not improve from 0.69315\n","Epoch 28/200\n","Learning rate is 0.0250.\n","74/74 [==============================] - 12s 168ms/step - loss: 0.6954 - accuracy: 0.5147 - val_loss: 0.6935 - val_accuracy: 0.5000\n","\n","Epoch 00028: val_loss did not improve from 0.69315\n","Epoch 29/200\n","Learning rate is 0.0250.\n","74/74 [==============================] - 13s 169ms/step - loss: 0.6943 - accuracy: 0.5084 - val_loss: 0.6945 - val_accuracy: 0.5000\n","\n","Epoch 00029: val_loss did not improve from 0.69315\n","Epoch 30/200\n","Learning rate is 0.0250.\n","74/74 [==============================] - 13s 170ms/step - loss: 0.6959 - accuracy: 0.4908 - val_loss: 0.7003 - val_accuracy: 0.5000\n","\n","Epoch 00030: val_loss did not improve from 0.69315\n","Epoch 31/200\n","Learning rate is 0.0250.\n","74/74 [==============================] - 13s 172ms/step - loss: 0.6951 - accuracy: 0.4928 - val_loss: 0.6968 - val_accuracy: 0.5000\n","\n","Epoch 00031: val_loss did not improve from 0.69315\n","Restoring model weights from the end of the best epoch.\n","Epoch 00031: early stopping\n","17/17 [==============================] - 0s 24ms/step - loss: 0.6931 - accuracy: 0.5000\n","Accuracy: 50.00%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ifTHvQcdc3Ew"},"source":["filepath = model_save_dir + \"pretrain.h5\"\n","model.save(filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7V4aOv0fUV_8"},"source":["## Redo Pre-Training on Loaded `pretrain.h5` Model (Optional)\n","\n","If you want to redo the pre-training step, be careful on the learning rate, because once you run `model.fit()`, the learning rate is changed w.r.t. the defined learning rate table. \n","\n","So, if you want to **continue** pre-training: \n","\n","- change the learning rate to `0.0125` and \n","- remove `CustomLearningRateScheduler(lr_schedule)` in `callbacks_list`. "]},{"cell_type":"code","metadata":{"id":"f928tM3dUZ4-"},"source":["filepath = model_save_dir + \"pretrain.h5\"\n","pretrain_model=load_model(filepath)\n","\n","history = pretrain_model.fit(X_train, Y_train,\n","                    batch_size=batch_size,\n","                    epochs=20,\n","                    verbose=1,\n","                    validation_data=(X_validation, Y_validation), \n","                    callbacks=callbacks_list,\n","                    shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ_bzunAFLSW"},"source":["## Making Plots of Accuracy and/or Loss"]},{"cell_type":"code","metadata":{"id":"ekBxX2SaTxzb"},"source":["# print(history.history.keys())\n","from matplotlib import pyplot as plt\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model loss and accuracy')\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","# save the plot "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EsU2xxYR1VM"},"source":["## Pre-Training Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Df-wb3vTTxZp"},"source":["### Evaluation Parameters Generation"]},{"cell_type":"code","metadata":{"id":"P20PmLDUuvVF"},"source":["model_filepath_pretrained = experiment_save_dir + 'pretrained.h5'\n","# evaluate the pretrained model\n","evaluation_model = load_model(model_filepath_pretrained) \n","# evaluation_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLXfvKX44NMT"},"source":["### Looking into the output of Softmax Layer (two dimensional vector)"]},{"cell_type":"code","metadata":{"id":"4TzP8Gw3wdv5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619166320004,"user_tz":-120,"elapsed":2538,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"253bf053-81f7-42b5-b32f-35e92cd921f7"},"source":["if verbose:\n","  model.summary()\n","Y_predict=model.predict(X_validation)\n","print('===============================')\n","print('|  prediction output   |  GT  |')\n","print('===============================')\n","for i in range(30):\n","  value = randint(0, X_validation.shape[0])\n","  print(str(Y_predict[value]) +'\\t'+ str(Y_validation[value]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv (Conv2D)                (None, 692, 1, 5)         2975      \n","_________________________________________________________________\n","globalPooling (GlobalAverage (None, 5)                 0         \n","_________________________________________________________________\n","FC1 (Dense)                  (None, 5)                 30        \n","_________________________________________________________________\n","FC2 (Dense)                  (None, 2)                 12        \n","=================================================================\n","Total params: 3,017\n","Trainable params: 3,017\n","Non-trainable params: 0\n","_________________________________________________________________\n","===============================\n","|  prediction output   |  GT  |\n","===============================\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.498934 0.501066]\t[1. 0.]\n","[0.49893358 0.5010664 ]\t[0. 1.]\n","[0.4989298 0.5010702]\t[1. 0.]\n","[0.49893412 0.5010659 ]\t[1. 0.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893397 0.501066  ]\t[0. 1.]\n","[0.4989339 0.5010661]\t[1. 0.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[1. 0.]\n","[0.4989339 0.5010661]\t[1. 0.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893412 0.5010659 ]\t[1. 0.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.4989341  0.50106597]\t[1. 0.]\n","[0.49893412 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893406 0.50106597]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[1. 0.]\n","[0.49893415 0.5010659 ]\t[1. 0.]\n","[0.49893412 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[0. 1.]\n","[0.49893415 0.5010659 ]\t[1. 0.]\n","[0.4989341 0.5010659]\t[0. 1.]\n","[0.49893406 0.5010659 ]\t[1. 0.]\n","[0.49893406 0.50106597]\t[1. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OhW4Ly2FTnCA"},"source":["### Looking into the Weights in Convolution Layer\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3I5K6BJET9-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619164708196,"user_tz":-120,"elapsed":644,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"e071be65-ca25-4e2f-b0b9-5c5cf076a8d8"},"source":["conv_layer = evaluation_model.get_layer('conv')\n","conv_layer_weights = conv_layer.get_weights()\n","print(\"the shape of weights in conv layer is: (\" + \\\n","      str(len(conv_layer_weights)) + ',' + \\\n","      str(len(conv_layer_weights[0])) + ',' + \\\n","      str(len(conv_layer_weights[0][0])) + ',' + \\\n","      str(len(conv_layer_weights[0][0][0])) + ',' + \\\n","      str(len(conv_layer_weights[0][0][0][0])) + ').')\n","conv_layer_weights = np.array(conv_layer_weights,dtype=object)[0]\n","# conv_layer_weights = np.array_split(conv_layer_weights, axis=4)\n","# for i in range(5):\n","conv_layer_weights_flatten=conv_layer_weights.flatten()\n","print('The mean of all weights is: ' + \n","      str(np.mean(conv_layer_weights_flatten)) + \n","      ', The variances of all weights is: ' + \n","      str(np.var(conv_layer_weights_flatten)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the shape of weights in conv layer is: (2,9,66,1,5).\n","The mean of all weights is: -0.18348497, The variances of all weights is: 45.345387\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zgRCj-i_Kl4X","executionInfo":{"status":"ok","timestamp":1619104919607,"user_tz":-120,"elapsed":665,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"0125f332-2d70-4fff-beea-bc9054f01f0a"},"source":["a=[[1,2,3,4],[2,2,3,4],[3,2,3,4],[4,2,3,4],[5,2,3,4]]\n","a[1][:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"markdown","metadata":{"id":"Cj8fGGNXUUn2"},"source":["# Retrain on Subject-Specific Manner"]},{"cell_type":"markdown","metadata":{"id":"pDoCVV53UN0F"},"source":["## Preparation for Subject-Specific Retraining"]},{"cell_type":"markdown","metadata":{"id":"xC_xtHboFOzm"},"source":["### split dataset into subject specific"]},{"cell_type":"code","metadata":{"id":"ZsYvoEcfo1Xf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079876255,"user_tz":-120,"elapsed":837,"user":{"displayName":"Chengbin Wang","photoUrl":"","userId":"10870159328381947884"}},"outputId":"7d4ddf7b-c7dc-4475-b76a-eae7e610cff0"},"source":["# do subject training and save accuracy\n","print('BEFORE: shape of training set  : ' + str(X_train.shape) +      ' and ' + str(Y_train.shape))\n","print('BEFORE: shape of validation set: ' + str(X_validation.shape) + ' and ' + str(Y_validation.shape))\n","X_train_per_subject = np.reshape(X_train, (number_of_subjects,\n","                                          int(X_train.shape[0]/number_of_subjects),\n","                                          X_train.shape[1],\n","                                          X_train.shape[2],\n","                                          X_train.shape[3]))\n","X_validation_per_subject = np.reshape(X_validation, (number_of_subjects, \n","                                          int(X_validation.shape[0]/number_of_subjects),\n","                                          X_validation.shape[1],\n","                                          X_validation.shape[2],\n","                                          X_validation.shape[3]))\n","Y_train_per_subject = np.reshape(Y_train, (number_of_subjects,\n","                                          int(Y_train.shape[0]/number_of_subjects),\n","                                          Y_train.shape[1]))\n","Y_validation_per_subject = np.reshape(Y_validation, (number_of_subjects,\n","                                          int(Y_validation.shape[0]/number_of_subjects),\n","                                          Y_validation.shape[1]))\n","# if verbose:\n","print('AFTER: shape of training set    : ' + str(X_train_per_subject.shape) + ' and ' + str(Y_train_per_subject.shape))\n","print('AFTER: shape of validation set  : ' + str(X_validation_per_subject.shape) + ' and ' + str(Y_validation_per_subject.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BEFORE: shape of training set  : (4816, 700, 66, 1) and (4816, 2)\n","BEFORE: shape of validation set: (688, 700, 66, 1) and (688, 2)\n","AFTER: shape of training set    : (16, 301, 700, 66, 1) and (16, 301, 2)\n","AFTER: shape of validation set  : (16, 43, 700, 66, 1) and (16, 43, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wV3_yevVbAQX"},"source":["## Subject-Specific Retraining Execution"]},{"cell_type":"code","metadata":{"id":"wJVlkvys1Jrd"},"source":["LR_SCHEDULE = [\n","    # (epoch to start, learning rate) tuples\n","    (0, 0.01),\n","    (10, 0.005),\n","    (25, 0.0025),\n","    (40, 0.00125),\n","]\n","scores=[]\n","subject_retrained_model=load_model(filepath)\n","filepath_retrain = model_save_dir + 'model_22_retrain.h5'\n","for subject_index in range(number_of_subjects):\n","  print('INFO: Subject retraining. Processing subject: ' + str(subject_index+1))\n","  X_train_one_subject      = np.reshape(X_train_per_subject[subject_index], \n","                                        (X_train_per_subject.shape[1],\n","                                         X_train_per_subject.shape[2],\n","                                         X_train_per_subject.shape[3],\n","                                         X_train_per_subject.shape[4],))\n","  X_validation_one_subject = np.reshape(X_validation_per_subject[subject_index], \n","                                        (X_validation_per_subject.shape[1],\n","                                         X_validation_per_subject.shape[2],\n","                                         X_validation_per_subject.shape[3],\n","                                         X_validation_per_subject.shape[4],))\n","  Y_train_one_subject      = np.reshape(Y_train_per_subject[subject_index],\n","                                        (Y_train_per_subject.shape[1],\n","                                         Y_train_per_subject.shape[2],))\n","  Y_validation_one_subject = np.reshape(Y_validation_per_subject[subject_index],\n","                                        (Y_validation_per_subject.shape[1],\n","                                         Y_validation_per_subject.shape[2],))\n","  if verbose:\n","    print(\"====> shape of training set    : \" + str(X_train_one_subject.shape) + ' and ' + str(Y_train_one_subject.shape))\n","    print(\"====> shape of validation set  : \" + str(X_validation_one_subject.shape) + ' and ' + str(Y_validation_one_subject.shape))\n","  checkpoint = ModelCheckpoint(filepath_retrain, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","  # callbacks_list = [CustomLearningRateScheduler(lr_schedule), checkpoint]\n","  callbacks_list = [CustomLearningRateScheduler(lr_schedule),]\n","  history = subject_retrained_model.fit(X_train_one_subject, Y_train_one_subject,\n","                    batch_size=int(batch_size/10),\n","                    epochs=50,\n","                    verbose=1,\n","                    validation_data=(X_validation_one_subject, Y_validation_one_subject), \n","                    callbacks=callbacks_list,\n","                    shuffle=True)\n","  scores.append(model.evaluate(X_validation_one_subject, Y_validation_one_subject, verbose=0))\n","  subject_retrained_model.save(filepath_retrain)\n","for i in range(16):\n","  print('Result: Accuracy of subject ' + str(i+1) + ': ' + str(scores[i][1]))\n","print('===================================================')\n","for i in range(16):\n","  print('Result: Loss     of subject ' + str(i+1) + ': ' + str(scores[i][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ioB3brP0w3Sz"},"source":["index_need_retrained = [5,6,8,10,11,12,15]\n","X_train_need_retrained =[]\n","X_validation_need_retrained=[]\n","Y_train_need_retrained=[]\n","Y_validation_need_retrained=[]\n","\n","def reshape_subject_X(input):\n","  output = np.array(input)\n","  output = np.reshape(output, (output.shape[0]*output.shape[1], output.shape[2],output.shape[3], output.shape[4]))\n","  return output\n","\n","def reshape_subject_Y(input):\n","  output = np.array(input)\n","  output = np.reshape(output, (output.shape[0]*output.shape[1], output.shape[2]))\n","  return output\n","\n","for subject_index in range(len(index_need_retrained)):\n","  print('INFO: Subject retraining. Processing subject: ' + str(subject_index+1))\n","  X_train_need_retrained.append(X_train_per_subject[index_need_retrained[subject_index]])\n","  X_validation_need_retrained.append(X_validation_per_subject[index_need_retrained[subject_index]])\n","  Y_train_need_retrained.append(Y_train_per_subject[index_need_retrained[subject_index]])\n","  Y_validation_need_retrained.append(Y_validation_per_subject[index_need_retrained[subject_index]])\n","\n","X_train_need_retrained = np.array(X_train_need_retrained)\n","X_validation_need_retrained = np.array(X_validation_need_retrained)\n","Y_train_need_retrained = np.array(Y_train_need_retrained)\n","Y_validation_need_retrained = np.array(Y_validation_need_retrained)\n","\n","X_train_need_retrained =reshape_subject_X(X_train_need_retrained)\n","X_validation_need_retrained=reshape_subject_X(X_validation_need_retrained)\n","Y_train_need_retrained=reshape_subject_Y(Y_train_need_retrained)\n","Y_validation_need_retrained=reshape_subject_Y(Y_validation_need_retrained)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjkdqFpCyaoU"},"source":["new_model=load_model(filepath_retrain)\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [CustomLearningRateScheduler(lr_schedule), checkpoint]\n","history = new_model.fit(X_train_need_retrained, Y_train_need_retrained,\n","                    batch_size=batch_size,\n","                    epochs=50,\n","                    verbose=2,\n","                    validation_data=(X_validation_need_retrained, Y_validation_need_retrained), \n","                    callbacks=callbacks_list,\n","                    shuffle=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMs0jfJf3pWp"},"source":["# model = load_model(filepath_retrain)\n","new_model.summary()\n","\n","for i \n","scores = new_model.evaluate(X_validation, Y_validation, verbose=verbose)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","Y_predict=new_model.predict(X_validation_need_retrained)\n","\n","for i in range(50):\n","  value = randint(0, X_validation_need_retrained.shape[0])\n","  print(str(Y_predict[value]) +'\\t'+ str(Y_validation[value]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vh6glD1OUh0V"},"source":["## Retraining: Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Lj0PiZMsUxCo"},"source":["# Overall Results"]},{"cell_type":"markdown","metadata":{"id":"uA6cykdIWifc"},"source":["## Test Result on Test Set"]},{"cell_type":"code","metadata":{"id":"c9TtlwkBWmDw"},"source":["if bool_test_set == False:\n","  pass\n"],"execution_count":null,"outputs":[]}]}